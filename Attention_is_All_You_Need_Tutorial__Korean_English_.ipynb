{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gangins/ex1/blob/main/Attention_is_All_You_Need_Tutorial__Korean_English_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfTcLf-rjaWd"
      },
      "source": [
        "#### **Attention is All You Need (NIPS 2017)** 실습\n",
        "* <b>(뉴스 데이터셋)</b> 한국어 문장을 영어 문장으로 번역합니다.\n",
        "* 본 코드는 기본적으로 **Transformer** 논문의 내용을 최대한 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델들은 본 논문에서 제안한 **Transformer 기반의 아키텍처**를 따르고 있습니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXXXOxNzeLpj"
      },
      "source": [
        "#### <b>한글 출력을 위한 폰트 설치</b>\n",
        "\n",
        "* 설치 이후에 수동으로 <b>[런타임]</b> - <b>[런타임 다시 시작]</b> 버튼을 눌러 재시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65nhszH6eNPt",
        "outputId": "36808590-ab3c-416d-a513-e5539237100d"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 1s (9,291 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJDFH3k03M6"
      },
      "source": [
        "#### <b>BLEU Score 계산을 위한 라이브러리 업데이트</b>\n",
        "\n",
        "* <b>[Restart Runtime]</b> 버튼을 눌러 런타임을 재시작할 필요가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrr-S31b031u",
        "outputId": "6667ebb2-5d31-4f9c-bf50-0d7dc3b448ca"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdH8Empjyxa"
      },
      "source": [
        "#### <b>한글 토큰화 라이브러리 설치하기</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2O0iSKl2mT_",
        "outputId": "f0ee1cf7-7a52-45ce-a8d5-673374a7be6d"
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 595 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5h73Lh9j2GW"
      },
      "source": [
        "#### <b>데이터셋 다운로드</b>\n",
        "\n",
        "* 한영 번역 데이터셋을 다운로드하여 파이썬 객체로 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdPZWIgl2oeF",
        "outputId": "ca7070eb-f4cc-4897-dd83-7b2890f087e1"
      },
      "source": [
        "# 한영 번역 데이터셋을 포함하는 저장소\n",
        "!git clone https://github.com/ndb796/korean-parallel-corpora"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'korean-parallel-corpora'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 0 (delta 0), pack-reused 131\u001b[K\n",
            "Receiving objects: 100% (131/131), 17.67 MiB | 24.58 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2iBk4-k2qR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f450be07-484d-4278-8fef-a852e96dc55e"
      },
      "source": [
        "# 데이터셋이 저장될 폴더 생성\n",
        "!mkdir -p ./dataset\n",
        "\n",
        "# 압축 해제\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.train.tar.gz -C ./dataset\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.test.tar.gz -C ./dataset\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.dev.tar.gz -C ./dataset\n",
        "\n",
        "# 학습(training) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.train.en ./dataset/train.en\n",
        "!mv ./dataset/korean-english-park.train.ko ./dataset/train.ko\n",
        "\n",
        "# 평가(validation) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.dev.en ./dataset/dev.en\n",
        "!mv ./dataset/korean-english-park.dev.ko ./dataset/dev.ko\n",
        "\n",
        "# 테스트(test) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.test.en ./dataset/test.en\n",
        "!mv ./dataset/korean-english-park.test.ko ./dataset/test.ko"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean-english-park.train.en\n",
            "korean-english-park.train.ko\n",
            "korean-english-park.test.en\n",
            "korean-english-park.test.ko\n",
            "korean-english-park.dev.en\n",
            "korean-english-park.dev.ko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_JbLlFqOpAo"
      },
      "source": [
        "#### <b>데이터셋 읽어 확인하기</b>\n",
        "\n",
        "* 학습, 평가, 테스트 데이터셋을 각각 읽어 문장 데이터를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTMM_Em2rUJ"
      },
      "source": [
        "korean_lines_train = open(\"./dataset/train.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_train = open(\"./dataset/train.en\", 'r', encoding='utf-8').readlines()\n",
        "\n",
        "korean_lines_val = open(\"./dataset/dev.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_val = open(\"./dataset/dev.en\", 'r', encoding='utf-8').readlines()\n",
        "\n",
        "korean_lines_test = open(\"./dataset/test.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_test = open(\"./dataset/test.en\", 'r', encoding='utf-8').readlines()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7PF8SFkBNv",
        "outputId": "41485603-5877-4a65-85a5-c116d9e9d533"
      },
      "source": [
        "print(f\"한글 문장 학습 데이터 개수: {len(korean_lines_train)}개\")\n",
        "print(f\"영어 문장 학습 데이터 개수: {len(english_lines_train)}개\")\n",
        "\n",
        "print(f\"한글 문장 평가 데이터 개수: {len(korean_lines_val)}개\")\n",
        "print(f\"영어 문장 평가 데이터 개수: {len(english_lines_val)}개\")\n",
        "\n",
        "print(f\"한글 문장 테스트 데이터 개수: {len(korean_lines_test)}개\")\n",
        "print(f\"영어 문장 테스트 데이터 개수: {len(english_lines_test)}개\")\n",
        "\n",
        "index = 777\n",
        "print(f\"{index + 1}번째 학습용 한글 문장:\", korean_lines_train[index], end='')\n",
        "print(f\"{index + 1}번째 학습용 영어 문장:\", english_lines_train[index], end='')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 문장 학습 데이터 개수: 94123개\n",
            "영어 문장 학습 데이터 개수: 94123개\n",
            "한글 문장 평가 데이터 개수: 1000개\n",
            "영어 문장 평가 데이터 개수: 1000개\n",
            "한글 문장 테스트 데이터 개수: 2000개\n",
            "영어 문장 테스트 데이터 개수: 2000개\n",
            "778번째 학습용 한글 문장: 지금 21살인 유는 학교에 가기 전 서너시간 동안 컴퓨터 통신에 끼어들기 위해 새벽 5시에 침대에서 일어나 나온다.\n",
            "778번째 학습용 영어 문장: Now Yu, 21, drags herself out of bed at 5 a.m. to squeeze in a few hours online before school.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcLKB3BRkDKW"
      },
      "source": [
        "#### <b>단어 사전 만들기 </b>\n",
        "\n",
        "* 단어 사전 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpaBry12tjd"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.UNK = '<unk>'\n",
        "        self.PAD = '<pad>'\n",
        "        self.SOS = '<sos>'\n",
        "        self.EOS = '<eos>'\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "    # 하나의 문장(sentence)에 포함된 모든 토큰을 추가하는 함수\n",
        "    def add_tokens(self, tokens):\n",
        "        for word in tokens:\n",
        "            if word in self.word2count:\n",
        "                self.word2count[word] += 1\n",
        "            else:\n",
        "                self.word2count[word] = 1\n",
        "\n",
        "    def preprocess(self, min_count):\n",
        "        # 사용하지 않을 단어 집합\n",
        "        trim_words = set()\n",
        "        for word, count in self.word2count.items():\n",
        "            if count < min_count:\n",
        "                trim_words.add(word)\n",
        "\n",
        "        # 실제로 사용할 단어만 남기기\n",
        "        words = set(self.word2count.keys()) - trim_words\n",
        "        words = [self.UNK, self.PAD, self.SOS, self.EOS] + list(words)\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        for i, word in enumerate(words):\n",
        "            self.word2idx[word] = i\n",
        "            self.idx2word[i] = word"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSB5vFULkecQ"
      },
      "source": [
        "#### <b>문장 토큰화</b>\n",
        "\n",
        "* 먼저 한글 문장 및 영어 문장 데이터셋에 대하여 토큰화를 수행합니다.\n",
        "* 토큰화를 위해 특수문자 제거 함수를 정의하고 객체를 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDH1f2gqlxO-"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def clean_string(string):\n",
        "    string = string.strip() # 앞뒤로 존재하는 공백 제거\n",
        "    string = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', string) # 특수문자 제거\n",
        "    return string.strip().lower() # 소문자로 변환하여 반환\n",
        "\n",
        "okt = Okt() # 한글 형태소 분석기"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fP80_AZmSC8"
      },
      "source": [
        "* 학습(training) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKq4u8pmRlM",
        "outputId": "06db72dc-cf7c-4400-a196-092c4b6d7f8d"
      },
      "source": [
        "tokenized_korean_lines_train = []\n",
        "tokenized_english_lines_train = []\n",
        "\n",
        "min_length = 4 # 단어의 개수가 4개 이상인 학습 문장 쌍만 사용\n",
        "max_length = 50 # 단어의 개수가 50개 이하인 학습 문장 쌍만 사용\n",
        "\n",
        "for i in range(len(korean_lines_train)):\n",
        "    korean = korean_lines_train[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_train[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    if len(korean_tokens) < min_length or len(korean_tokens) > max_length:\n",
        "        continue\n",
        "    if len(english_tokens) < min_length or len(english_tokens) > max_length:\n",
        "        continue\n",
        "\n",
        "    tokenized_korean_lines_train.append(korean_tokens)\n",
        "    tokenized_english_lines_train.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 4000 == 0:\n",
        "        print(f\"학습 데이터셋 토큰화: {i + 1}/{len(korean_lines_train)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터셋 토큰화: 4000/94123\n",
            "학습 데이터셋 토큰화: 8000/94123\n",
            "학습 데이터셋 토큰화: 12000/94123\n",
            "학습 데이터셋 토큰화: 16000/94123\n",
            "학습 데이터셋 토큰화: 20000/94123\n",
            "학습 데이터셋 토큰화: 24000/94123\n",
            "학습 데이터셋 토큰화: 28000/94123\n",
            "학습 데이터셋 토큰화: 32000/94123\n",
            "학습 데이터셋 토큰화: 36000/94123\n",
            "학습 데이터셋 토큰화: 40000/94123\n",
            "학습 데이터셋 토큰화: 44000/94123\n",
            "학습 데이터셋 토큰화: 48000/94123\n",
            "학습 데이터셋 토큰화: 52000/94123\n",
            "학습 데이터셋 토큰화: 56000/94123\n",
            "학습 데이터셋 토큰화: 60000/94123\n",
            "학습 데이터셋 토큰화: 64000/94123\n",
            "학습 데이터셋 토큰화: 68000/94123\n",
            "학습 데이터셋 토큰화: 72000/94123\n",
            "학습 데이터셋 토큰화: 76000/94123\n",
            "학습 데이터셋 토큰화: 80000/94123\n",
            "학습 데이터셋 토큰화: 84000/94123\n",
            "학습 데이터셋 토큰화: 88000/94123\n",
            "학습 데이터셋 토큰화: 92000/94123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerYg_C1nOk7"
      },
      "source": [
        "* 평가(validation) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKrnkrbTnPoc",
        "outputId": "d6f863d8-c199-45ae-d3ac-6105c5645050"
      },
      "source": [
        "tokenized_korean_lines_val = []\n",
        "tokenized_english_lines_val = []\n",
        "\n",
        "for i in range(len(korean_lines_val)):\n",
        "    korean = korean_lines_val[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_val[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    tokenized_korean_lines_val.append(korean_tokens)\n",
        "    tokenized_english_lines_val.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"평가 데이터셋 토큰화: {i + 1}/{len(korean_lines_val)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평가 데이터셋 토큰화: 1000/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_sP1Wn3pvQU"
      },
      "source": [
        "* 테스트(test) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxY4lieapyQj",
        "outputId": "4daf426d-e0d9-4bcf-cc86-325a37c1c4d4"
      },
      "source": [
        "tokenized_korean_lines_test = []\n",
        "tokenized_english_lines_test = []\n",
        "\n",
        "for i in range(len(korean_lines_test)):\n",
        "    korean = korean_lines_test[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_test[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    tokenized_korean_lines_test.append(korean_tokens)\n",
        "    tokenized_english_lines_test.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"테스트 데이터셋 토큰화: {i + 1}/{len(korean_lines_test)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터셋 토큰화: 1000/2000\n",
            "테스트 데이터셋 토큰화: 2000/2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxvJN5THq2hx"
      },
      "source": [
        "#### <b>단어 사전 만들기</b>\n",
        "\n",
        "* 최소 2번 이상 등장한 단어만 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNVKZ7NbrP2l",
        "outputId": "1a03a381-28ce-4b66-c67a-fafa8841fdb6"
      },
      "source": [
        "korean_voca = Vocabulary()\n",
        "english_voca = Vocabulary()\n",
        "\n",
        "for i in range(len(tokenized_korean_lines_train)):\n",
        "    korean_tokens = tokenized_korean_lines_train[i]\n",
        "    english_tokens = tokenized_english_lines_train[i]\n",
        "\n",
        "    korean_voca.add_tokens(korean_tokens)\n",
        "    english_voca.add_tokens(english_tokens)\n",
        "\n",
        "korean_voca.preprocess(min_count=2)\n",
        "english_voca.preprocess(min_count=2)\n",
        "\n",
        "print(\"전체 한국어 단어 수:\", len(korean_voca.word2count))\n",
        "print(\"전체 영어 단어 수:\", len(english_voca.word2count))\n",
        "print(\"사용할 한국어 토큰 수:\", len(korean_voca.word2idx))\n",
        "print(\"사용할 영어 토큰 수:\", len(english_voca.word2idx))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 한국어 단어 수: 67029\n",
            "전체 영어 단어 수: 58833\n",
            "사용할 한국어 토큰 수: 40612\n",
            "사용할 영어 토큰 수: 35745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wyKQjwAusyw",
        "outputId": "19f71234-e61a-42df-8d4c-ea29ad08478b"
      },
      "source": [
        "print(korean_voca.word2idx['<pad>']) # 패딩(padding): 1\n",
        "print(korean_voca.word2idx['<sos>']) # <sos>: 2\n",
        "print(korean_voca.word2idx['<eos>']) # <eos>: 3\n",
        "print(korean_voca.word2idx['컴퓨터'])\n",
        "print(korean_voca.word2idx['사랑'])\n",
        "print(korean_voca.word2idx['기적'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "6570\n",
            "7273\n",
            "24531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjD1bjVuvHt",
        "outputId": "9e088e2d-24ef-4ab5-da76-59a8527b0e5b"
      },
      "source": [
        "print(english_voca.word2idx['<pad>']) # 패딩(padding): 1\n",
        "print(english_voca.word2idx['<sos>']) # <sos>: 2\n",
        "print(english_voca.word2idx['<eos>']) # <eos>: 3\n",
        "print(english_voca.word2idx['computer'])\n",
        "print(english_voca.word2idx['love'])\n",
        "print(english_voca.word2idx['miracle'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "30345\n",
            "5627\n",
            "14389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw7jo7dmsHRW"
      },
      "source": [
        "* Unknown Token이 1개 이상 포함된 문장은 데이터셋에서 제외하여 다시 학습 데이터셋을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M717ROWBsFmM"
      },
      "source": [
        "unknown_threshold = 1\n",
        "\n",
        "preprocessed_korean_lines_train = []\n",
        "preprocessed_english_lines_train = []\n",
        "\n",
        "for i in range(len(tokenized_korean_lines_train)):\n",
        "    korean_tokens = tokenized_korean_lines_train[i]\n",
        "    english_tokens = tokenized_english_lines_train[i]\n",
        "\n",
        "    is_used = True # 현재의 문장 쌍을 사용할지의 여부\n",
        "    for token in korean_tokens:\n",
        "        cnt = 0\n",
        "        if token not in korean_voca.word2idx:\n",
        "            cnt += 1\n",
        "        if cnt >= unknown_threshold:\n",
        "            is_used = False\n",
        "    for token in english_tokens:\n",
        "        cnt = 0\n",
        "        if token not in english_voca.word2idx:\n",
        "            cnt += 1\n",
        "        if cnt >= unknown_threshold:\n",
        "            is_used = False\n",
        "\n",
        "    if not is_used:\n",
        "        continue\n",
        "\n",
        "    preprocessed_korean_lines_train.append(korean_tokens)\n",
        "    preprocessed_english_lines_train.append(english_tokens)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg6lc1V4uDQ6",
        "outputId": "a759b9cb-9a3f-4e0e-885b-f37ba5af9330"
      },
      "source": [
        "print(\"사용할 한국어 학습 문장 수:\", len(preprocessed_korean_lines_train))\n",
        "print(\"사용할 영어 학습 문장 수:\", len(preprocessed_english_lines_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용할 한국어 학습 문장 수: 61029\n",
            "사용할 영어 학습 문장 수: 61029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH1SIu_i5mch",
        "outputId": "a78c6db9-7c5f-42c0-b8cc-9f605c5bdaf3"
      },
      "source": [
        "print(preprocessed_korean_lines_train[7777])\n",
        "print(preprocessed_english_lines_train[7777])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cnn', '의', '여론조사', '국장', '인', '키팅', '홀랜드', '는', '“', '이라크전', '발발', '직후', '부시', '의', '지지도', '는', '71', '였다', '”', '며', '“', '지지율', '40', '추락', '은', '베트남전', '당시', '린', '든', '존슨', '대통령', '과', '유사하다', '”', '고', '지적', '했다']\n",
            "['bushs', 'approval', 'rating', 'five', 'years', 'ago', 'at', 'the', 'start', 'of', 'the', 'iraq', 'war', 'was', '71', 'percent', 'and', 'that', '40point', 'drop', 'is', 'almost', 'identical', 'to', 'the', 'drop', 'president', 'lyndon', 'johnson', 'faced', 'during', 'the', 'vietnam', 'war', 'said', 'cnn', 'polling', 'director', 'keating', 'holland']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eItQjid_uzsB"
      },
      "source": [
        "#### <b>커스텀 데이터셋 클래스 작성하기</b>\n",
        "\n",
        "* 소스 문장(한국어)과 타겟 문장(영어)를 한 쌍으로 반환하는 데이터셋 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C7JV5PE5rmM"
      },
      "source": [
        "import copy\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, korean_lines, english_lines, max_seq_len):\n",
        "        self.korean_lines = korean_lines\n",
        "        self.english_lines = english_lines\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoder_input = self.get_encoder_input(self.korean_lines[index])\n",
        "        decoder_input = self.get_decoder_input(self.english_lines[index])\n",
        "\n",
        "        return encoder_input, decoder_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.korean_lines)\n",
        "\n",
        "    # 한글 문장 벡터화\n",
        "    def get_encoder_input(self, tokens):\n",
        "        tokens = copy.deepcopy(tokens)\n",
        "        tokens.insert(0, korean_voca.SOS)\n",
        "        tokens.append(korean_voca.EOS)\n",
        "        tokens = self.padding(tokens, korean_voca) # 문장 뒤쪽에 패딩 붙이기\n",
        "        index_list = self.word2idx(tokens, korean_voca)\n",
        "\n",
        "        return torch.tensor(index_list).to(device)\n",
        "\n",
        "    # 영어 문장 벡터화\n",
        "    def get_decoder_input(self, tokens):\n",
        "        tokens = copy.deepcopy(tokens)\n",
        "        tokens.insert(0, english_voca.SOS)\n",
        "        tokens.append(english_voca.EOS)\n",
        "        tokens = self.padding(tokens, english_voca) # 문장 뒤쪽에 패딩 붙이기\n",
        "        index_list = self.word2idx(tokens, english_voca)\n",
        "\n",
        "        return torch.tensor(index_list).to(device)\n",
        "\n",
        "    # max_seq_len보다 길이가 짧은 문장에 대해 <pad> 토큰 채우기\n",
        "    def padding(self, tokens, voca):\n",
        "        if len(tokens) < self.max_seq_len:\n",
        "            tokens += [voca.PAD] * (self.max_seq_len - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        return tokens\n",
        "\n",
        "    def word2idx(self, tokens, voca):\n",
        "        idx_list = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                idx_list.append(voca.word2idx[token])\n",
        "            except KeyError:\n",
        "                idx_list.append(voca.word2idx[voca.UNK])\n",
        "        return idx_list"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EObNRkSSvPfk"
      },
      "source": [
        "* 학습/평가/테스트 데이터셋 객체를 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBsC-CRc5_F7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = CustomDataset(preprocessed_korean_lines_train, preprocessed_english_lines_train, max_seq_len=80)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128, num_workers=0)\n",
        "\n",
        "val_dataset = CustomDataset(tokenized_korean_lines_val, tokenized_english_lines_val, max_seq_len=80)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=128, num_workers=0)\n",
        "\n",
        "test_dataset = CustomDataset(tokenized_korean_lines_test, tokenized_english_lines_test, max_seq_len=80)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=128, num_workers=0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlKQSsxK6BdK",
        "outputId": "ab4ad636-5f19-4caa-e3d9-f5e7d8a43412"
      },
      "source": [
        "# 하나의 배치에 포함되어 있는 문장을 출력합니다.\n",
        "for i, batch in enumerate(train_loader):\n",
        "    src = batch[0]\n",
        "    trg = batch[1]\n",
        "\n",
        "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
        "\n",
        "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
        "    for i in range(src.shape[1]):\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
        "\n",
        "    # 첫 번째 배치만 확인\n",
        "    break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 배치 크기: torch.Size([128, 80])\n",
            "인덱스 0: 2\n",
            "인덱스 1: 3662\n",
            "인덱스 2: 673\n",
            "인덱스 3: 15679\n",
            "인덱스 4: 24046\n",
            "인덱스 5: 24934\n",
            "인덱스 6: 34622\n",
            "인덱스 7: 32590\n",
            "인덱스 8: 24934\n",
            "인덱스 9: 8967\n",
            "인덱스 10: 19153\n",
            "인덱스 11: 4172\n",
            "인덱스 12: 31163\n",
            "인덱스 13: 21164\n",
            "인덱스 14: 3523\n",
            "인덱스 15: 32590\n",
            "인덱스 16: 5975\n",
            "인덱스 17: 13422\n",
            "인덱스 18: 7415\n",
            "인덱스 19: 40360\n",
            "인덱스 20: 1539\n",
            "인덱스 21: 27280\n",
            "인덱스 22: 6159\n",
            "인덱스 23: 33530\n",
            "인덱스 24: 11723\n",
            "인덱스 25: 33456\n",
            "인덱스 26: 3\n",
            "인덱스 27: 1\n",
            "인덱스 28: 1\n",
            "인덱스 29: 1\n",
            "인덱스 30: 1\n",
            "인덱스 31: 1\n",
            "인덱스 32: 1\n",
            "인덱스 33: 1\n",
            "인덱스 34: 1\n",
            "인덱스 35: 1\n",
            "인덱스 36: 1\n",
            "인덱스 37: 1\n",
            "인덱스 38: 1\n",
            "인덱스 39: 1\n",
            "인덱스 40: 1\n",
            "인덱스 41: 1\n",
            "인덱스 42: 1\n",
            "인덱스 43: 1\n",
            "인덱스 44: 1\n",
            "인덱스 45: 1\n",
            "인덱스 46: 1\n",
            "인덱스 47: 1\n",
            "인덱스 48: 1\n",
            "인덱스 49: 1\n",
            "인덱스 50: 1\n",
            "인덱스 51: 1\n",
            "인덱스 52: 1\n",
            "인덱스 53: 1\n",
            "인덱스 54: 1\n",
            "인덱스 55: 1\n",
            "인덱스 56: 1\n",
            "인덱스 57: 1\n",
            "인덱스 58: 1\n",
            "인덱스 59: 1\n",
            "인덱스 60: 1\n",
            "인덱스 61: 1\n",
            "인덱스 62: 1\n",
            "인덱스 63: 1\n",
            "인덱스 64: 1\n",
            "인덱스 65: 1\n",
            "인덱스 66: 1\n",
            "인덱스 67: 1\n",
            "인덱스 68: 1\n",
            "인덱스 69: 1\n",
            "인덱스 70: 1\n",
            "인덱스 71: 1\n",
            "인덱스 72: 1\n",
            "인덱스 73: 1\n",
            "인덱스 74: 1\n",
            "인덱스 75: 1\n",
            "인덱스 76: 1\n",
            "인덱스 77: 1\n",
            "인덱스 78: 1\n",
            "인덱스 79: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bieO3YwVvmLY"
      },
      "source": [
        "#### **Multi Head Attention 아키텍처**\n",
        "\n",
        "* 어텐션(attention)은 <b>세 가지 요소</b>를 입력으로 받습니다.\n",
        "    * <b>쿼리(queries)</b>\n",
        "    * <b>키(keys)</b>\n",
        "    * <b>값(values)</b>\n",
        "    * 현재 구현에서는 Query, Key, Value의 차원이 모두 같습니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRX0AoF1voKW"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hidden_dim % n_heads == 0\n",
        "\n",
        "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
        "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # query: [batch_size, query_len, hidden_dim]\n",
        "        # key: [batch_size, key_len, hidden_dim]\n",
        "        # value: [batch_size, value_len, hidden_dim]\n",
        " \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Q: [batch_size, query_len, hidden_dim]\n",
        "        # K: [batch_size, key_len, hidden_dim]\n",
        "        # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
        "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "        # K: [batch_size, n_heads, key_len, head_dim]\n",
        "        # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # Attention Energy 계산\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        # energy: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 마스크(mask)를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        return x, attention"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSDUlmOvq6-"
      },
      "source": [
        "#### **Position-wise Feedforward 아키텍처**\n",
        "\n",
        "* 입력과 출력의 차원이 동일합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBfNsiED6LSe"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        # x: [batch_size, seq_len, pf_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6fWXndvvwJ"
      },
      "source": [
        "#### **인코더(Encoder) 레이어 아키텍처**\n",
        "\n",
        "* 하나의 인코더 레이어에 대해 정의합니다.\n",
        "    * 입력과 출력의 차원이 같습니다.\n",
        "    * 이러한 특징을 이용해 트랜스포머의 인코더는 인코더 레이어를 여러 번 중첩해 사용합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "* &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwhr1-df6MKk"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 하나의 임베딩이 복제되어 Query, Key, Value로 입력되는 방식\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # position-wise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N22uqlEmvx06"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 전체 인코더 아키텍처를 정의합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "    * **max_length**: 문장 내 최대 단어 개수\n",
        "* 원본 논문과는 다르게 <b>위치 임베딩(positional embedding)을 학습</b>하는 형태로 구현합니다.\n",
        "    * BERT와 같은 모던 트랜스포머 아키텍처에서 사용되는 방식입니다.\n",
        "* &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j2ZRWaf6M14"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, src_len]\n",
        "\n",
        "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src # 마지막 레이어의 출력을 반환"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-eh0mAvzva"
      },
      "source": [
        "#### **디코더(Decoder) 레이어 아키텍처**\n",
        "\n",
        "* 하나의 디코더 레이어에 대해 정의합니다.\n",
        "    * 입력과 출력의 차원이 같습니다.\n",
        "    * 이러한 특징을 이용해 트랜스포머의 디코더는 디코더 레이어를 여러 번 중첩해 사용합니다.\n",
        "    * 디코더 레이어에서는 두 개의 Multi-Head Attention 레이어가 사용됩니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "* 소스 문장의 &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다.\n",
        "* 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBalCQMq6Nj4"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 자기 자신에 대하여 어텐션(attention)\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # encoder attention\n",
        "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return trg, attention"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kRYUk-1v1jv"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "* 전체 디코더 아키텍처를 정의합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **output_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "    * **max_length**: 문장 내 최대 단어 개수\n",
        "* 원본 논문과는 다르게 <b>위치 임베딩(positional embedding)을 학습</b>하는 형태로 구현합니다.\n",
        "    * BERT와 같은 모던 트랜스포머 아키텍처에서 사용되는 방식입니다.\n",
        "* Seq2Seq과는 마찬가지로 실제로 추론(inference) 시기에서는 디코더를 반복적으로 넣을 필요가 있습니다.\n",
        "    * 학습(training) 시기에서는 한 번에 출력 문장을 구해 학습할 수 있습니다.\n",
        "* 소스 문장의 &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다.\n",
        "* 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJ-DCDt6Oev"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, trg_len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # 소스 마스크와 타겟 마스크 모두 사용\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        output = self.fc_out(trg)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn-yCx4yv348"
      },
      "source": [
        "#### **트랜스포머(Transformer) 아키텍처**\n",
        "\n",
        "* 최종적인 전체 트랜스포머(Transformer) 모델을 정의합니다.\n",
        "* 입력이 들어왔을 때 앞서 정의한 인코더와 디코더를 거쳐 출력 문장을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ignEkCL6PSr"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
        "    def make_src_mask(self, src):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
        "    def make_trg_mask(self, trg):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        \"\"\"\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 1 0\n",
        "        1 1 1 1 1\n",
        "        \"\"\"\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "        # trg_sub_mask: [trg_len, trg_len]\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1TDOflzv5xW"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ombHNgO56Q7r"
      },
      "source": [
        "INPUT_DIM = len(korean_voca.word2idx)\n",
        "OUTPUT_DIM = len(english_voca.word2idx)\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-Tr-D06SbJ"
      },
      "source": [
        "SRC_PAD_IDX = korean_voca.word2idx[korean_voca.PAD]\n",
        "TRG_PAD_IDX = english_voca.word2idx[english_voca.PAD]\n",
        "\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1DbVBkVwCvA"
      },
      "source": [
        "* **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XObg1hxB6bOA",
        "outputId": "0748ead5-0f35-407b-b9e7-80e7ec4879f4"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 32,738,721 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWItEfu26chy",
        "outputId": "f1230492-c40c-4adb-d1fc-568501b4be58"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(40612, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(35745, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=35745, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Je4gMnSwITT"
      },
      "source": [
        "* 학습 및 평가 함수 정의\n",
        "    * 기본적인 Seq2Seq 모델과 거의 유사하게 작성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_hn64HD6dYK"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg6MC3dK6gF9"
      },
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch[0]\n",
        "        trg = batch[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEukz7xc6o6F"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "            # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기, trg_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4-D3IuNwO9t"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuAulfy_6pow"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC-dgh1E6qbF",
        "outputId": "283f2bc3-48b0-4931-d1db-567cfbba914e"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 4\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer_korean_to_english.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 44s\n",
            "\tTrain Loss: 6.452 | Train PPL: 633.762\n",
            "\tValidation Loss: 6.013 | Validation PPL: 408.646\n",
            "Epoch: 02 | Time: 2m 48s\n",
            "\tTrain Loss: 5.108 | Train PPL: 165.260\n",
            "\tValidation Loss: 5.649 | Validation PPL: 283.883\n",
            "Epoch: 03 | Time: 2m 50s\n",
            "\tTrain Loss: 4.396 | Train PPL: 81.134\n",
            "\tValidation Loss: 5.531 | Validation PPL: 252.506\n",
            "Epoch: 04 | Time: 2m 50s\n",
            "\tTrain Loss: 3.817 | Train PPL: 45.448\n",
            "\tValidation Loss: 5.569 | Validation PPL: 262.284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1ntWEV6uth"
      },
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(korean, model, device, max_len=80, logging=True):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    korean = clean_string(korean)\n",
        "    tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [korean_voca.SOS] + tokens + [korean_voca.EOS]\n",
        "    if logging:\n",
        "        print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            src_indexes.append(korean_voca.word2idx[token])\n",
        "        except KeyError:\n",
        "            src_indexes.append(korean_voca.word2idx[korean_voca.UNK])\n",
        "    if logging:\n",
        "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # 소스 문장에 따른 마스크 생성\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [english_voca.word2idx[english_voca.SOS]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        # 출력 문장에 따른 마스크 생성\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # 출력 문장에서 가장 마지막 단어만 사용\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == english_voca.word2idx[english_voca.EOS]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [english_voca.idx2word[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avx76dMr9mXm",
        "outputId": "be32fc0d-82c2-47ba-d69a-b3977c67962a"
      },
      "source": [
        "example_idx = 15\n",
        "\n",
        "src = korean_lines_test[example_idx]\n",
        "trg = english_lines_test[example_idx]\n",
        "\n",
        "print(f'소스 문장: {src}', end='')\n",
        "print(f'타겟 문장: {trg}', end='')\n",
        "\n",
        "translation, attention = translate_sentence(src, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: 도쿄의 니케이 지수는 9291.03으로 0.33 퍼센트 하락했다.\n",
            "타겟 문장: Tokyo's Nikkei is off 0.33 percent to 9291.03.\n",
            "전체 소스 토큰: ['<sos>', '도쿄', '의', '니', '케이', '지수', '는', '929103', '으로', '033', '퍼센트', '하락', '했다', '<eos>']\n",
            "소스 문장 인덱스: [2, 1675, 38880, 17675, 23008, 23373, 15679, 0, 12450, 0, 1487, 17301, 33456, 3]\n",
            "모델 출력 결과: tokyo japan cnn tokyo has been down with the kospi index <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2360KqGAcHNX"
      },
      "source": [
        "* 어텐션 맵(Attention Map) 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEhp1bcE33uU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
        "\n",
        "    assert n_rows * n_cols == n_heads\n",
        "\n",
        "    plt.rc('font', family='NanumBarunGothic') # 폰트 설정\n",
        "    fig = plt.figure(figsize=(15, 25)) # 출력할 그림 크기 조절\n",
        "\n",
        "    for i in range(n_heads):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
        "\n",
        "        # 어텐션(Attention) 스코어 확률 값을 이용해 그리기\n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'], rotation=45)\n",
        "        ax.set_yticklabels([''] + translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzlH8RZEcIJF",
        "outputId": "56e5c912-06ec-4a51-d5a7-74123bbb62cc"
      },
      "source": [
        "src = \"남한과 미국은 동맹적인 관계를 유지하고 있다.\"\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "\n",
        "translation, attention = translate_sentence(src, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: 남한과 미국은 동맹적인 관계를 유지하고 있다.\n",
            "전체 소스 토큰: ['<sos>', '남한', '과', '미국', '은', '동맹', '적', '인', '관계', '를', '유지', '하고', '있다', '<eos>']\n",
            "소스 문장 인덱스: [2, 16059, 21164, 16150, 31175, 1425, 8784, 40088, 13976, 19608, 3622, 5067, 11667, 3]\n",
            "모델 출력 결과: south korea and the united states have a diplomatic solution <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dnwiCsNlcKkU",
        "outputId": "956e3a87-ec81-460a-8276-be4c3321a6c8"
      },
      "source": [
        "src = clean_string(src)\n",
        "korean_tokens = [line[0] for line in okt.pos(src, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "display_attention(korean_tokens, translation, attention)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45224 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44284 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48120 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44397 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51008 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46041 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47609 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51064 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44288 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44228 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47484 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50976 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51648 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45224 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44284 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48120 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44397 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51008 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46041 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47609 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51064 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44288 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44228 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47484 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50976 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51648 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1800 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdEp459kf2Bz"
      },
      "source": [
        "#### <b>BLEU Score 계산</b>\n",
        "\n",
        "* 학습된 트랜스포머(Transformer) 모델의 BLEU 스코어 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76pGgi0IcS6M"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def show_bleu(model, device, max_len=80):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    index = 0\n",
        "\n",
        "    for i in range(len(korean_lines_test)):\n",
        "        src = korean_lines_test[i]\n",
        "        trg = english_lines_test[i]\n",
        "\n",
        "        trg = clean_string(trg)\n",
        "        trg = trg.split(' ')\n",
        "\n",
        "        pred_trg, _ = translate_sentence(src, model, device, max_len, logging=False)\n",
        "\n",
        "        # 마지막 <eos> 토큰 제거\n",
        "        pred_trg = pred_trg[:-1]\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "\n",
        "        index += 1\n",
        "        if (index + 1) % 100 == 0:\n",
        "            print(f\"[{index + 1}/{len(korean_lines_test)}]\")\n",
        "            print(f\"예측: {pred_trg}\")\n",
        "            print(f\"정답: {trg}\")\n",
        "\n",
        "    bleu = bleu_score(pred_trgs, trgs, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "    print(f'Total BLEU Score = {bleu*100:.2f}')\n",
        "\n",
        "    individual_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "    individual_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 1, 0, 0])\n",
        "    individual_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 1, 0])\n",
        "    individual_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 0, 1])\n",
        "\n",
        "    print(f'Individual BLEU1 score = {individual_bleu1_score*100:.2f}') \n",
        "    print(f'Individual BLEU2 score = {individual_bleu2_score*100:.2f}') \n",
        "    print(f'Individual BLEU3 score = {individual_bleu3_score*100:.2f}') \n",
        "    print(f'Individual BLEU4 score = {individual_bleu4_score*100:.2f}') \n",
        "\n",
        "    cumulative_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "    cumulative_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/2, 1/2, 0, 0])\n",
        "    cumulative_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/3, 1/3, 1/3, 0])\n",
        "    cumulative_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/4, 1/4, 1/4, 1/4])\n",
        "\n",
        "    print(f'Cumulative BLEU1 score = {cumulative_bleu1_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU2 score = {cumulative_bleu2_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU3 score = {cumulative_bleu3_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU4 score = {cumulative_bleu4_score*100:.2f}') "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuh4aVgCf3MU",
        "outputId": "715d1a82-1c17-4be3-9bbb-bb4ae4707032"
      },
      "source": [
        "show_bleu(model, device)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100/2000]\n",
            "예측: ['the', 'discovery', 'of', 'the', 'discovery', 'of', 'the', 'ancient', 'ancient', 'ancient', 'ancient', 'roman', 'african', 'union', 'said', 'that', 'the', 'discovery', 'of', 'the', 'images', 'are', 'not', 'known', 'as', 'the', 'discovery', 'of', 'the', 'ancient', 'ancient', 'ancient', 'ancient', 'ancient', 'ancient', 'ancient', 'ancient', 'art']\n",
            "정답: ['the', 'skull', 'nicknamed', 'toumai', 'found', 'in', 'northern', 'chad', 'by', 'an', 'international', 'team', 'is', 'not', 'part', 'of', 'the', 'human', 'family', 'tree']\n",
            "[200/2000]\n",
            "예측: ['the', 'boy', 'was', 'found', 'unconscious', 'and', 'a', 'knife', 'wound', 'that', 'was', 'found', 'dead', 'and', 'a', 'man', 'who', 'was', 'shot', 'dead']\n",
            "정답: ['johnsons', 'eviscerated', 'body', 'which', 'police', 'said', 'they', 'found', 'after', 'receiving', 'calls', 'about', 'a', 'foul', 'odor', 'coming', 'from', 'the', 'apartment', 'was', 'in', 'a', 'state', 'of', 'moderate', 'decomposition', 'and', 'she', 'had', 'been', 'dead', 'about', 'two', 'days', 'medical', 'examiner', 'karl', 'williams', 'said']\n",
            "[300/2000]\n",
            "예측: ['the', 'results', 'of', 'the', 'results', 'of', 'the', '700', 'billion', 'were', 'not', 'known', 'as', 'the', 'result', 'of', 'the', 'worlds', 'largest', 'population']\n",
            "정답: ['studies', 'show', 'productivity', 'drops', 'when', 'temperatures', 'dip', 'below', '72', 'degrees']\n",
            "[400/2000]\n",
            "예측: ['he', 'was', 'arrested', 'on', 'charges', 'of', 'illegally', 'and', 'charged', 'with', 'attempted', 'murder', 'conspiracy', 'to', 'commit', 'murder']\n",
            "정답: ['the', 'couple', 'who', 'face', 'charges', 'of', 'bodily', 'harm', 'making', 'threats', 'and', 'coercion', 'were', 'released', 'on', 'bail', 'thursday', 'he', 'said']\n",
            "[500/2000]\n",
            "예측: ['israel', 'has', 'been', 'charged', 'with', 'attempted', 'murder', 'conspiracy', 'to', 'commit', 'suicide']\n",
            "정답: ['israel', 'on', 'wednesday', 'also', 'released', 'the', 'remains', 'of', '199', 'fighters', 'from', 'lebanon']\n",
            "[600/2000]\n",
            "예측: ['the', 'us', 'geological', 'survey', 'struck', 'a', 'magnitude', 'of', 'the', 'coast', 'guard', 'near', 'the', 'coast', 'of', 'the', 'coast', 'guard', 'and', 'the', 'coast', 'guard', 'said']\n",
            "정답: ['a', 'strong', 'earthquake', 'struck', 'monday', 'near', 'some', 'greek', 'islands', 'close', 'to', 'the', 'turkish', 'coast', 'according', 'to', 'the', 'us', 'geological', 'service']\n",
            "[700/2000]\n",
            "예측: ['the', 'announcement', 'came', 'after', 'the', 'last', 'month', 'was', 'released', 'from', 'the', 'last', 'week', 'of', 'a', 'deal', 'which', 'was', 'delayed', 'by', 'the', 'company']\n",
            "정답: ['inbevs', 'original', 'offer', 'of', '46', 'billion', 'made', 'on', 'june', '11th', 'was', 'rejected', 'as', 'too', 'low']\n",
            "[800/2000]\n",
            "예측: ['but', 'the', 'club', 'said', 'it', 'was', 'a', 'perfect', 'mirror', 'to', 'get', 'back', 'to', 'the', 'product', 'and', 'then', 'it', 'was', 'a', 'lot', 'of', 'pounds']\n",
            "정답: ['but', 'she', 'points', 'out', 'that', 'the', 'jewelry', 'is', 'almost', 'always', 'made', 'from', 'cheap', 'metal', 'that', 'will', 'turn', 'yellow', 'or', 'lose', 'its', 'sheen', 'within', 'weeks']\n",
            "[900/2000]\n",
            "예측: ['iran', 'has', 'been', 'a', 'russian', 'missile', 'defense', 'ministry', 'spokesman', 'said', 'the', 'russian', 'defense', 'system', 'was', 'not', 'immediately', 'known', 'as', 'a', 'missile', 'defense', 'system']\n",
            "정답: ['iranian', 'media', 'reported', 'that', 'tehran', 'fired', 'a', 'series', 'of', 'missiles', 'on', 'thursday', 'in', 'a', 'second', 'day', 'of', 'longrange', 'missile', 'testing']\n",
            "[1000/2000]\n",
            "예측: ['police', 'said', 'the', 'woman', 'was', 'trying', 'to', 'kill', 'her', 'and', 'that', 'the', 'men', 'were', 'killed', 'in', 'a', 'raid', 'on', 'a', 'bus', 'in', 'the', 'town', 'of', 'areopolis', 'near', 'the', 'town', 'of', 'areopolis', 'located', 'about', '20', 'miles', 'away']\n",
            "정답: ['when', 'police', 'surrounded', 'the', 'men¡¯s', 'apartment', 'tuesday', 'they', 'found', '15', 'men', 'and', 'women', 'wielding', 'knives', 'and', 'shouting', '¡°sacrifice', 'for', 'allah¡±', 'a', 'police', 'spokesman', 'told', 'xinhua']\n",
            "[1100/2000]\n",
            "예측: ['north', 'korea', 'has', 'refused', 'to', 'return', 'to', 'pyongyang', 'after', 'the', 'communist', 'regime', 'was', 'released']\n",
            "정답: ['north', 'korea', 'declared', 'details', 'of', 'its', 'nuclear', 'program', 'last', 'month']\n",
            "[1200/2000]\n",
            "예측: ['president', 'george', 'w', 'bush', 'said', 'he', 'was', 'pleased', 'with', 'the', 'president', 'bush', 'administration']\n",
            "정답: ['president', 'bush', 'gave', 'a', 'positive', 'but', 'cautious', 'assessment', 'of', 'russia¡¯s', 'new', 'president', 'dmitry', 'medvedev']\n",
            "[1300/2000]\n",
            "예측: ['he', 'also', 'said', 'he', 'would', 'not', 'be', 'able', 'to', 'produce', 'the', 'worlds', 'most', 'famous', 'meters', 'of', 'the', 'worlds', 'most', 'hated']\n",
            "정답: ['von', 'hagens', 'says', 'he', 'relies', 'on', 'donors', 'not', 'only', 'as', 'a', 'source', 'of', 'specimens', 'but', 'also', 'as', 'representations', 'of', 'body', 'worlds', 'philosophy']\n",
            "[1400/2000]\n",
            "예측: ['ma', 'yingjeou', 'has', 'been', 'elected', 'as', 'a', 'major', 'victory', 'of', 'the', 'dalai', 'lama']\n",
            "정답: ['taiwans', 'new', 'president', 'ma', 'yingjeou', 'has', 'rejected', 'the', 'push', 'for', 'independence']\n",
            "[1500/2000]\n",
            "예측: ['walter', 'hill', 'won', 'the', 'new', 'york', 'times', 'the', 'new', 'york', 'times', 'the', 'film', 'festival', 'which', 'is', 'being', 'made', 'for', 'the', 'first', 'time', 'in', 'new', 'york']\n",
            "정답: ['walter', 'scott', '24', 'put', 'his', 'soul', 'up', 'for', 'sale', 'on', 'new', 'zealand', 'internet', 'auction', 'site', 'trademe', 'and', 'so', 'far', 'has', 'received', 'more', 'than', '100', 'expressions', 'of', 'interest']\n",
            "[1600/2000]\n",
            "예측: ['the', 'us', 'ambassador', 'to', 'washington', 'has', 'been', 'criticized', 'for', 'talks', 'with', 'north', 'korea']\n",
            "정답: ['but', 'the', 'governing', 'uri', 'party', 'still', 'underlined', 'the', 'importance', 'of', 'interkorean', 'economic', 'projects']\n",
            "[1700/2000]\n",
            "예측: ['authorities', 'say', 'the', 'bodies', 'of', 'a', 'helicopter', 'was', 'in', 'the', 'area', 'where', 'the', 'victims', 'were', 'killed']\n",
            "정답: ['regional', 'officials', 'said', 'he', 'died', 'in', 'a', 'shootout', 'when', 'marines', 'swooped', 'on', 'his', 'hideout', 'in', 'the', 'island', 'of', 'tawitawi']\n",
            "[1800/2000]\n",
            "예측: ['the', 'un', 'secretary', 'of', 'state', 'condoleezza', 'rice', 'has', 'been', 'trying', 'to', 'take', 'a', 'third', 'term', 'as', 'the', 'un']\n",
            "정답: ['the', 'move', 'was', 'opposed', 'by', 'the', 'party', 'of', 'president', 'nicolas', 'sarkozy', 'who', 'has', 'been', 'trying', 'to', 'ease', 'tense', 'ties', 'with', 'beijing']\n",
            "[1900/2000]\n",
            "예측: ['the', 'dow', 'is', 'up', '45', 'points']\n",
            "정답: ['right', 'now', 'the', 'dow', 'is', 'up', '14', 'points']\n",
            "[2000/2000]\n",
            "예측: ['japan', 'has', 'killed', 'two', 'people', 'in', 'the', 'train', 'bombings']\n",
            "정답: ['japans', 'derailed', 'commuter', 'train', 'accident', 'has', 'killed', 'at', 'least', '69', 'people']\n",
            "Total BLEU Score = 1.93\n",
            "Individual BLEU1 score = 19.64\n",
            "Individual BLEU2 score = 3.32\n",
            "Individual BLEU3 score = 0.82\n",
            "Individual BLEU4 score = 0.26\n",
            "Cumulative BLEU1 score = 19.64\n",
            "Cumulative BLEU2 score = 8.07\n",
            "Cumulative BLEU3 score = 3.77\n",
            "Cumulative BLEU4 score = 1.93\n"
          ]
        }
      ]
    }
  ]
}